"""
Module de benchmark pour comparer plusieurs mod√®les sur un m√™me jeu de donn√©es.
Supporte la parall√©lisation et l'affichage de la progression.
"""
import time
from typing import Dict, Any, Optional
from tqdm import tqdm
from joblib import Parallel, delayed
from .evaluation import Evaluator


def _train_and_evaluate(name, model, X_train, y_train, X_test, y_test):
    """
    Fonction helper pour entra√Æner et √©valuer un mod√®le unique.
    Utilis√©e pour la parall√©lisation.
    
    Returns:
        tuple: (nom, r√©sultats)
    """
    # Mesure du temps d'entra√Ænement
    start_fit = time.time()
    model.fit(X_train, y_train)
    fit_time = time.time() - start_fit

    # Mesure du temps de pr√©diction
    start_pred = time.time()
    y_pred = model.predict(X_test)
    predict_time = time.time() - start_pred

    scores = Evaluator.evaluate_all(y_test, y_pred)
    return name, {
        'scores': scores,
        'fit_time': fit_time,
        'predict_time': predict_time
    }


class Benchmark:
    """
    Classe pour comparer les performances de plusieurs mod√®les de classification/r√©gression.
    
    Supporte:
    - Ex√©cution s√©quentielle ou parall√®le (via joblib)
    - Barre de progression (via tqdm)
    - Mesure des temps d'entra√Ænement et de pr√©diction
    """
    def __init__(self, models: Dict[str, Any]):
        """
        Args:
            models (dict): dictionnaire {nom: instance_modele}
        """
        self.models = models
        self.results = None

    def run(
        self,
        X_train,
        y_train,
        X_test,
        y_test,
        parallel: bool = False,
        n_jobs: int = -1,
        show_progress: bool = True
    ) -> Dict[str, Dict]:
        """
        Entra√Æne et √©value chaque mod√®le, retourne les scores et les temps d'ex√©cution.
        
        Args:
            X_train: Donn√©es d'entra√Ænement
            y_train: Cibles d'entra√Ænement
            X_test: Donn√©es de test
            y_test: Cibles de test
            parallel (bool): Si True, ex√©cute les mod√®les en parall√®le (d√©faut: False)
            n_jobs (int): Nombre de jobs pour la parall√©lisation (-1 = tous les c≈ìurs)
            show_progress (bool): Si True, affiche une barre de progression (d√©faut: True)
        
        Returns:
            dict: {nom_modele: {scores, fit_time, predict_time}}
        """
        results = {}
        
        if parallel:
            # Ex√©cution parall√®le avec joblib
            model_items = list(self.models.items())
            
            if show_progress:
                print(f"üöÄ Benchmark parall√®le de {len(model_items)} mod√®les...")
            
            parallel_results = Parallel(n_jobs=n_jobs)(
                delayed(_train_and_evaluate)(
                    name, model, X_train, y_train, X_test, y_test
                )
                for name, model in tqdm(
                    model_items,
                    desc="Entra√Ænement",
                    disable=not show_progress
                )
            )
            
            for name, res in parallel_results:
                results[name] = res
        else:
            # Ex√©cution s√©quentielle avec barre de progression
            iterator = self.models.items()
            if show_progress:
                iterator = tqdm(
                    iterator,
                    total=len(self.models),
                    desc="Benchmark",
                    unit="mod√®le"
                )
            
            for name, model in iterator:
                if show_progress:
                    iterator.set_postfix({"mod√®le": name})
                
                # Mesure du temps d'entra√Ænement
                start_fit = time.time()
                model.fit(X_train, y_train)
                fit_time = time.time() - start_fit

                # Mesure du temps de pr√©diction
                start_pred = time.time()
                y_pred = model.predict(X_test)
                predict_time = time.time() - start_pred

                scores = Evaluator.evaluate_all(y_test, y_pred)
                results[name] = {
                    'scores': scores,
                    'fit_time': fit_time,
                    'predict_time': predict_time
                }
        
        self.results = results
        return results
    
    def summary(self) -> Optional[str]:
        """
        Retourne un r√©sum√© format√© des r√©sultats du benchmark.
        
        Returns:
            str: R√©sum√© textuel des r√©sultats, ou None si pas de r√©sultats
        """
        if self.results is None:
            return None
        
        lines = ["=" * 60, "üìä R√âSUM√â DU BENCHMARK", "=" * 60]
        
        # Trouver le meilleur mod√®le par accuracy
        best_model = None
        best_accuracy = -1
        
        for name, res in self.results.items():
            lines.append(f"\nüîπ {name}")
            lines.append("-" * 40)
            for metric, value in res['scores'].items():
                lines.append(f"  {metric}: {value:.4f}")
            lines.append(f"  ‚è±Ô∏è fit_time: {res['fit_time']:.4f}s")
            lines.append(f"  ‚è±Ô∏è predict_time: {res['predict_time']:.4f}s")
            
            if res['scores'].get('accuracy', 0) > best_accuracy:
                best_accuracy = res['scores'].get('accuracy', 0)
                best_model = name
        
        if best_model:
            lines.append("\n" + "=" * 60)
            lines.append(f"üèÜ MEILLEUR MOD√àLE: {best_model} (accuracy: {best_accuracy:.4f})")
            lines.append("=" * 60)
        
        return "\n".join(lines)
    
    def print_summary(self):
        """Affiche le r√©sum√© du benchmark."""
        summary = self.summary()
        if summary:
            print(summary)
        else:
            print("‚ö†Ô∏è Aucun r√©sultat. Ex√©cutez d'abord run().")
